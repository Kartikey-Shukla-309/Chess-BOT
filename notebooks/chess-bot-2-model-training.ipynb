{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14333991,"sourceType":"datasetVersion","datasetId":9151506}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **MODEL TRAINING**","metadata":{}},{"cell_type":"markdown","source":"## **NEURAL-NETWORK ARCHITECTURE -** \n\nThe architecture is inspired by modern neural chess engines and enables the system to both select strong moves and evaluate positions effectively without relying on traditional search algorithms.\n\nThis model uses a Residual Convolutional Neural Network because CNNs give good results where there need for detecting patterns like image classification and in this case a chess game. The output layers are Dense layers with two output heads: a policy head that predicts probabilities over 4672 possible chess moves and a value head that evaluates the position. \n\nResidual connections allow deeper feature learning without gradient degradation which is very common in chess games like, the common openings are played very frequently, hindering the learning process. The convolutional layers learn how pieces should be placed on the board, cordination between pieces and identify other patterns.","metadata":{}},{"cell_type":"markdown","source":"The input is reshaped to 8 x 8 x 18 matrix because for maximum time of the game same coloured pieces are on same side of the board and while applying filter, it should see the same coloured pieces together to learn positional play","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**HELPER FUNCTIONS FROM \"Chess_BOT_1_Data_Processing\"**","metadata":{}},{"cell_type":"code","source":"# Numeric representation of each piece 'a'- for white pieces and 'A'- for black pieces\n\n''' In this I went a bit out of convention. This is the correct convention 'A'- for white \n    and 'a'- for black.\n    In later part the correct convention is followed and in code 'piece_weight' are multiplied \n    with a extra '-1' to rectify the error.\n'''\n\nnum_piece = {'p':0,'n':1,'b':2,'r':3,'q':4,'k':5,\n             'P':6,'N':7,'B':8,'R':9,'Q':10,'K':11}\n\npiece_weight = {'p':1,'n':3,'b':3,'r':5,'q':9,'k':0,\n             'P':-1,'N':-3,'B':-3,'R':-5,'Q':-9,'K':0}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extracting each move as FEN from PGN\n\ndef pgn_to_fen(PGN):\n  fen = []\n  pgn = io.StringIO(PGN)\n  game = chess.pgn.read_game(pgn)\n  for move in game.mainline_moves():\n    fen.append(b.fen())\n  return fen","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extracting each move as FEN from PGN\n\ndef pgn_to_fen(PGN):\n  fen = []\n  pgn = io.StringIO(PGN)\n  game = chess.pgn.read_game(pgn)\n  for move in game.mainline_moves():\n    fen.append(b.fen())\n  return fen","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# converting a FEN to a 12x8x8 matrix: 8x8 for board and x12 for each type of chess piece\n\ndef fen_to_matrix(FEN):\n  matrix = np.zeros((12,8,8))\n  b = chess.Board(FEN)\n  for square,piece in b.piece_map().items():\n    r = 7 - square//8\n    c = square%8\n    matrix[num_piece[str(piece)],r,c] = 1\n  return matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating material points\n\ndef material_points(FEN):\n  white_point = 0\n  black_point = 0\n\n  b = chess.Board(FEN)\n  for square,piece in b.piece_map().items():\n    if str(piece).isupper():\n      white_point = white_point + piece_weight[str(piece)]\n    elif str(piece).islower():\n      black_point = black_point + piece_weight[str(piece)]\n  return white_point,black_point","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  additional features to the board matrix\n\ndef add_board(matrix,turn,FEN):\n\n  # side to move\n  side_plane = np.ones((1,8,8)) * turn\n\n  # castling rights\n  castle = []\n  if(board.has_kingside_castling_rights(chess.WHITE)):\n    castling_plane = castle.append(np.ones((8,8)))\n    castling_plane = castle.append(np.zeros((8,8)))\n  else:\n    castling_plane = castle.append(np.zeros((8,8)))\n    castling_plane = castle.append(np.ones((8,8)))\n  if(board.has_kingside_castling_rights(chess.BLACK)):\n    castling_plane = castle.append(np.ones((8,8)))\n    castling_plane = castle.append(np.zeros((8,8)))\n  else:\n    castling_plane = castle.append(np.zeros((8,8)))\n    castling_plane = castle.append(np.ones((8,8)))\n\n  # material points\n  white_point,black_point = material_points(FEN)\n  material_advantage = black_point + white_point\n  material = np.full((1, 8, 8),material_advantage,dtype=np.float32) * (-1*turn)\n\n  add_matrix = np.concatenate([matrix, side_plane, castle, material], axis=0)\n\n  return add_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**POLICY LABEL -** Encoding move played in a particular position. Makes an array of arrays of length 4672(a single move) to record all moves of a game. The value at index of move (0-4671) played in a particular position is made 1 and rest all 0.","metadata":{}},{"cell_type":"code","source":"# Gives index(out of 4672) for the move played in a position\n\ndef move_played_idx(move):\n\n  # indexing move\n  from_sq = move.from_square\n  to_sq = move.to_square\n  from_rank = chess.square_rank(from_sq)\n  from_file = chess.square_file(from_sq)\n  to_rank = chess.square_rank(to_sq)\n  to_file = chess.square_file(to_sq)\n\n  d_file = to_file - from_file\n  d_rank = to_rank - from_rank\n\n  # sliding moves\n  slide_dir = [(0,1),(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1)]\n\n  for dir_idx,(dx,dy) in enumerate(slide_dir):\n    for dist in range(1,8):\n        if d_file == dx*dist and d_rank == dy*dist:\n        move_type = dir_idx*7 + dist - 1\n        return from_sq*73 + move_type\n\n  # knight move\n  knight_dir = [(1,2),(2,1),(2,-1),(1,-2),(-1,-2),(-2,-1),(-2,1),(-1,2)]\n  for dir_idx,(dx,dy) in enumerate(knight_dir):\n    if d_file == dx and d_rank == dy:\n      move_type = 56 + dir_idx\n      return from_sq*73 + move_type\n\n  # underpromotion\n  if move.promotion is not None:\n    promo_map = {chess.KNIGHT:0,chess.BISHOP:1,chess.ROOK:2}\n    if move.promotion in promo_map:\n      is_capture = (d_file != 0)\n      move_type = 64 + (is_capture*3) + promo_map[move.promotion]\n      return from_sq*73 + move_type\n\n  return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Policy label for a complete game\n\ndef policy_label(PGN):\n    pgn = io.StringIO(PGN)\n    game = chess.pgn.read_game(pgn)\n    policy = []\n\n    for move in game.mainline_moves():\n        m = np.zeros(4672, dtype=np.float32)\n        idx = move_played_idx(move)\n        m[idx] = 1.0\n        policy.append(m)\n\n    policy = np.stack(policy, axis=0)\n    return policy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**VALUE LABEL -** Encoding the evaluation of a position as the result of the game. White won = +1, Black won = -1, Draw = 0. It is a scalar and its value in each position is assumed to be same as the result of the game. It is assumed that at high level moves played are generally good, their chances of making blunders is less. This is not a good practice for position evaluation but can be considered fine at beginner level ","metadata":{}},{"cell_type":"code","source":"# Value label \n\ndef value_label(result,turn):\n    return result * turn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **CREATING BATCHES -**\nThe model will be trained on positions i.e. give a particular position as matrix and map it to policy and value labels.\n\nA single game contains 30+ moves so training can't be performed at once. Therefore, model is trained in batches. ","metadata":{}},{"cell_type":"code","source":"# Processing a single game: from PGN extract FENs and form policy and value label for each FEN\n\ndef process_single_game(pgn, result):\n    fens = pgn_to_fen(pgn)\n    policies = policy_label(pgn)\n\n    X_list, P_list, V_list = [], [], []\n\n    turn = 1;\n    for fen, policy in zip(fens, policies):\n        #board = chess.Board(fen)\n        #turn = 1 if board.turn else -1\n\n        piece_matrix = fen_to_matrix(fen)\n        input_tensor = add_board(piece_matrix, turn, fen)\n\n        value = value_label(result, turn)\n        turn = -1 * turn\n\n        X_list.append(input_tensor)\n        P_list.append(policy)\n        V_list.append(value)\n\n    return (\n        np.asarray(X_list, dtype=np.float32),\n        np.asarray(P_list, dtype=np.float32),\n        np.asarray(V_list, dtype=np.float32)\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a batch of games\n\ndef game_batch_to_position_batch(df_batch):\n    X_all = []\n    policy_all = []\n    value_all = []\n\n    for _, row in df_batch.iterrows():\n        Xg, Pg, Vg = process_single_game(row['PGN'], row['Result'])\n\n        X_all.append(Xg)\n        policy_all.append(Pg)\n        value_all.append(Vg)\n\n    X = np.concatenate(X_all, axis=0)\n    Y_policy = np.concatenate(policy_all, axis=0)\n    Y_value = np.concatenate(value_all, axis=0)\n\n    return X, Y_policy, Y_value","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a batch of positions from multiple games\n\ndef train_generator(train_df, start_game=16, games_per_batch=8, shuffle=False):\n    n = len(train_df)\n\n    # Decide order ONCE\n    if shuffle:\n        df = train_df.sample(frac=1).reset_index(drop=True)\n    else:\n        df = train_df.reset_index(drop=True)\n\n    current = start_game\n\n    while current < n:\n        df_batch = df.iloc[current: current + games_per_batch]\n\n        X, Y_policy, Y_value = game_batch_to_position_batch(df_batch)\n\n        yield X, {\n            \"policy_head\": Y_policy,\n            \"value_head\": Y_value\n        }\n\n        current += games_per_batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **BUILDING A MODEL**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating Residual block\n\ndef residual_block(x, filters, l2_reg=3e-4):\n    shortcut = x\n\n    x = layers.Conv2D(\n        filters,\n        kernel_size=3,\n        padding='same',\n        use_bias=False,\n        kernel_regularizer=regularizers.l2(l2_reg)\n    )(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    x = layers.Conv2D(\n        filters,\n        kernel_size=3,\n        padding='same',\n        use_bias=False,\n        kernel_regularizer=regularizers.l2(l2_reg)\n    )(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Add()([shortcut, x])\n    x = layers.ReLU()(x)\n\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Building chess model\n\ndef build_chess_model(\n    input_shape=(18, 8, 8),\n    num_res_blocks=4,\n    num_filters=128,\n    policy_size=4672\n):\n    \n    inputs = layers.Input(shape=input_shape)\n\n    x = layers.Permute((2, 3, 1))(inputs)  # (8, 8, 18)\n\n    x = layers.Conv2D(\n        num_filters,\n        kernel_size=3,\n        padding='same',\n        use_bias=False,\n        kernel_regularizer=regularizers.l2(1e-4)\n    )(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    for _ in range(num_res_blocks):\n        x = residual_block(x, num_filters)\n\n    # Policy Head\n\n    p = layers.Conv2D(\n        2,\n        kernel_size=1,\n        use_bias=False,\n        kernel_regularizer=regularizers.l2(1e-4)\n    )(x)\n    p = layers.BatchNormalization()(p)\n    p = layers.ReLU()(p)\n    p = layers.Flatten()(p)\n    p = layers.Dense(\n        policy_size,\n        activation='softmax',\n        name='policy_head'\n    )(p)\n\n    # Value Head\n\n    v = layers.Conv2D(\n        1,\n        kernel_size=1,\n        use_bias=False,\n        kernel_regularizer=regularizers.l2(1e-4)\n    )(x)\n    v = layers.BatchNormalization()(v)\n    v = layers.ReLU()(v)\n    v = layers.Flatten()(v)\n    v = layers.Dense(\n        256,\n        activation='relu',\n        kernel_regularizer=regularizers.l2(1e-4)\n    )(v)\n    v = layers.Dense(\n        1,\n        activation='tanh',\n        name='value_head'\n    )(v)\n\n    model = models.Model(inputs=inputs, outputs=[p, v])\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compiling model\n\nmodel = build_chess_model(\n    num_res_blocks=4,\n    num_filters=128\n)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    loss={\n        \"policy_head\": tf.keras.losses.CategoricalCrossentropy(),\n        \"value_head\": tf.keras.losses.MeanSquaredError()\n    },\n    loss_weights={\n        \"policy_head\": 1.0,\n        \"value_head\": 1.0\n    }\n)\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **TRAINING MODEL**","metadata":{}},{"cell_type":"markdown","source":"Train on first batch and Save and Download model","metadata":{}},{"cell_type":"code","source":"# Loading DataFrame\n\ntrain_df = pd.read_pickle('/kaggle/input/chess-train-dataset/train_df.pkl')\n\nprint(train_df.shape)\nprint(type(train_df))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fitting model\n\nmodel.fit(\n    train_generator(train_df, games_per_batch=8),\n    steps_per_epoch=len(train_df) // 8,\n    epochs=2\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, Yp, Yv = game_batch_to_position_batch(train_df.iloc[:2])\n\nprint(X.shape)\nprint(Yp.shape)\nprint(Yv.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport os\n\nSAVE_DIR = \"/kaggle/working\"\nmodel.save(os.path.join(SAVE_DIR, \"chess_model.keras\"))\n\n!ls /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**LOAD, TRAIN, SAVE, DOWNLOAD** and repeat this process for entire dataset. Keep incrementing the value of 'start_game' by 'games_per_batch' each time before training","metadata":{}}]}